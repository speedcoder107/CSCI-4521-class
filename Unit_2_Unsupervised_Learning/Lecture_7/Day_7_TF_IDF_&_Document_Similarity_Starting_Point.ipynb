{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knrJAx-PSAvw"
      },
      "source": [
        "## A simple tf-idf implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "PtL4-UajRvu6"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def tfidf(term, doc, doc_list):\n",
        "  docs_with_term = [doc for doc in doc_list if term in doc]\n",
        "  tf = doc.count(term) #term count\n",
        "  df = len(docs_with_term)/len(doc_list) #document frequency\n",
        "  idf = math.log(1/df)\n",
        "  return tf * idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "5PfCjzZ6XqSB"
      },
      "outputs": [],
      "source": [
        "documents = [\"the mouse\", \"the small cat\", \"the cheese\", \"the big cat\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "zBwOLaztXu6T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the importance is 1.3862943611198906\n",
            "the importance is 1.3862943611198906\n",
            "the importance is 1.3862943611198906\n",
            "the importance is 0.6931471805599453\n"
          ]
        }
      ],
      "source": [
        "# TODO: calculate the importance of each word in a document\n",
        "print(\"the importance is\", tfidf(\"mouse\", \"the mouse\", documents))\n",
        "print(\"the importance is\", tfidf(\"small\", \"the small cat\", documents))\n",
        "print(\"the importance is\", tfidf(\"cheese\", \"the cheese\", documents))\n",
        "print(\"the importance is\", tfidf(\"cat\", \"the big cat\", documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "bzcCa2ChI2fS"
      },
      "outputs": [],
      "source": [
        "def getUniqueVocab(documents):\n",
        "  #Find unique vocab words\n",
        "  vocab = []\n",
        "  for doc in documents:\n",
        "    vocab.extend(doc.split())\n",
        "  vocab = set(vocab)\n",
        "  return vocab\n",
        "\n",
        "def vectorize(phrase, documents):\n",
        "  #Find unique vocab words\n",
        "  vocab = getUniqueVocab(documents)\n",
        "\n",
        "  #Build vector representation\n",
        "  result = []\n",
        "  for word in vocab:\n",
        "    result.append(tfidf(word, phrase, documents))\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "qJpm_m8YZir-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'big', 'mouse', 'cat', 'cheese', 'the', 'small'}\n",
            "[0.0, 0.0, 0.6931471805599453, 0.0, 0.0, 1.3862943611198906]\n"
          ]
        }
      ],
      "source": [
        "# TODO: lets vectorize some documents!\n",
        "print(getUniqueVocab(documents))\n",
        "print(vectorize(\"the small cat\", documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egPqe9z8SFbj"
      },
      "source": [
        "----\n",
        "## TF-IDF with SK-Learn\n",
        "\n",
        "This is a pretty cude approach to tf-idf. In practice we often want to smooth out the tf-idf computation, and include extra normalization terms.\n",
        "\n",
        "There are many corner cases to consider, and variations on how we can compute the `tf` term and the `idf` term.\n",
        "\n",
        "Its often useful to use a library to compute tf-idf, and one is provided in `SK Learn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "IZ3jr_oDTGko"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "jvFwJvs5byu9"
      },
      "outputs": [],
      "source": [
        "# TODO: create vectorizer and fit_transform our documents (training set)\n",
        "vectorizer = TfidfVectorizer(min_df=1)\n",
        "vectorized_docs = vectorizer.fit_transform(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSKuQxxnU2Uc"
      },
      "source": [
        "Note that \"the\" (word #5) has a non-zero value even though it occurs in every document. \"The\" is still the smallest values (around 0.4), and \"cheese\"/\"mouse\" still the largest value (0.88).\n",
        "\n",
        "For which examples does SK-Learn's TF-IDF formula give \"the\" the most weight? Why is that? Any advantages to this method?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DddBkh1Tz0U7"
      },
      "source": [
        "actual sklearn formulat : tfidf = log(N/df) * ln(1+M/T) where N is the number of occurrences of the word in the corpus, Df is the total number of documents in the collection, M is the number of times the word occurs in this document, and T is the total number of words in this particular document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDc34rLQXb8j"
      },
      "source": [
        "We can use `vectorizer.transform` along with `toarray()` to get the vectorized result of a new document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "vGrOmpsoVexJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.66338461 0.         0.66338461 0.         0.         0.34618161]]\n"
          ]
        }
      ],
      "source": [
        "# TODO: print the vector version of \"the big cheese\"\n",
        "print(vectorizer.transform([\"the big cheese\"]).toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSxIYoNhb12G"
      },
      "source": [
        "----\n",
        "## Your turn:\n",
        "\n",
        "1. First, can you compute the distance between two vectors. My examples give the sqaured Euclidean distance, but you can use other distance terms.\n",
        "\n",
        "Here is a stub of `my_dist` to get you started"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "25V0ci0zi8Uv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def my_dist(v1, v2):\n",
        "  return np.sum( (v1-v2)**2 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "xmucfoDPcFMn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "25\n"
          ]
        }
      ],
      "source": [
        "print(my_dist(np.array([3]),np.array([4])))          #Squared L2 Dist is 1\n",
        "print(my_dist(np.array([1,1]),np.array([2,2])))      #Squared L2 Dist is 2\n",
        "print(my_dist(np.array([1,2,3]),np.array([1,-1,7]))) #Squared L2 Dist is 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTM39fCncsWE"
      },
      "source": [
        "If you above distance function works, you can now take the distance between every vector and an new document using code like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "gGrrK1e7ZLiy"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[np.float64(1.6796869262748297),\n",
              " np.float64(1.7374616297112078),\n",
              " np.float64(0.5034428121791692),\n",
              " np.float64(0.7733760581012943)]"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_post_vec = vectorizer.transform([\"the big cheese\"])\n",
        "[my_dist(new_post_vec.toarray(),train_vec.toarray()) for train_vec in vectorized_docs] #I get [1.7, 1.7, 0.5, 0.7], your results may differ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xWTjbL1daCK"
      },
      "source": [
        "2. Now write a function called `findClosest` which takes as input a string, and returns a string of the document in `documents` which has the closest feature vector.\n",
        "\n",
        "Again, we'll give you a stub of `findClosest` to help get you started:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "gHMo3t6Ui6Em"
      },
      "outputs": [],
      "source": [
        "def findClosest(promt):\n",
        "  # TODO: Fix this to be the actual closest index like you find closest in nearest meighbors algorithm  \n",
        "  closest_id = 0\n",
        "  min_dist = float('inf')\n",
        "  for i, vec in enumerate(vectorized_docs):\n",
        "    dist = my_dist(new_post_vec.toarray(), vec.toarray())\n",
        "    if dist < min_dist:\n",
        "      min_dist = dist\n",
        "      closest_id = i\n",
        "  return documents[closest_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "7MSJyWj7YccJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'the cheese'"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "findClosest(\"the big cheese\") #Should probably return \"the cheese\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtsC8pt6gNJ0"
      },
      "source": [
        "---\n",
        "Thought Experiment - Submit your answers as your activity!\n",
        " - What would you want to happen when the input is `\"a cheesy slice of pizza\"`\n",
        " - What does happen?\n",
        " - How might we fix this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "stSuIFw7gWJr"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'the cheese'"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "findClosest(\"a cheesy slice of pizza\")  #TODO: Should this stil return \"the cheese\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Yes, it does."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (CSCI-4521)",
      "language": "python",
      "name": "csci-4521-class-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
